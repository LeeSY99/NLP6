{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **필요한 라이브러리 및 모듈**"
      ],
      "metadata": {
        "id": "BF8pQHp5MH4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-_ijmhbLrRL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import VotingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **캐글 데이터 불러오기**"
      ],
      "metadata": {
        "id": "h7QNO0HzMif5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽어오기\n",
        "# 대회에서 제공된 train, test data\n",
        "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
        "sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
        "org_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n",
        "\n",
        "# 데이터 증강을 통해 얻은 train data\n",
        "train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')"
      ],
      "metadata": {
        "id": "KIeY73tRLz79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **전처리**"
      ],
      "metadata": {
        "id": "DF7mOxhBM1Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train 데이터 중복 제거\n",
        "train = train.drop_duplicates(subset=['text'])\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# tokenizer 관련 설정\n",
        "LOWERCASE = False # 대소문자 설정\n",
        "VOCAB_SIZE = 30522 # 어휘 크기 설정\n",
        "\n",
        "# tokenizer 인스턴스 설정\n",
        "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
        "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel() # byte 단위로 분할\n",
        "\n",
        "# 특수토큰, trainer 설정\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
        "\n",
        "# 토크나이저에 적합한 형태로 데이터셋 생성\n",
        "dataset = Dataset.from_pandas(test[['text']])\n",
        "\n",
        "# 데이터셋을 반복문으로 토크나이져 학습\n",
        "def train_corp_iter():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]\n",
        "\n",
        "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
        "\n",
        "# PreTrainedTokenizerFast를 사용하여 토크나이저 설정\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=raw_tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")\n",
        "\n",
        "# 테스트셋 텍스트 토큰화\n",
        "tokenized_texts_test = []\n",
        "for text in tqdm(test['text'].tolist()):\n",
        "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
        "\n",
        "# 트레인셋 텍스트 토큰화\n",
        "tokenized_texts_train = []\n",
        "for text in tqdm(train['text'].tolist()):\n",
        "    tokenized_texts_train.append(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "9KNTS7HoL4Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **단어 벡터화**"
      ],
      "metadata": {
        "id": "xdMWpw7eM9pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer 단어 벡터화\n",
        "def dummy(text):\n",
        "    \"\"\"\n",
        "    이미 토큰화 한 텍스트를 그대로 백터화 하기 위해서 설정\n",
        "    \"\"\"\n",
        "    return text\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
        "    tokenizer = dummy,\n",
        "    preprocessor = dummy,\n",
        "    token_pattern = None, strip_accents='unicode')\n",
        "\n",
        "# 토큰화 된 텍스트들로 벡터화 학습\n",
        "vectorizer.fit(tokenized_texts_test)\n",
        "\n",
        "vocab = vectorizer.vocabulary_\n",
        "# ngram 범위 (3, 5)로 설정\n",
        "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n",
        "                            analyzer = 'word',\n",
        "                            tokenizer = dummy,\n",
        "                            preprocessor = dummy,\n",
        "                            token_pattern = None, strip_accents='unicode'\n",
        "                            )"
      ],
      "metadata": {
        "id": "JC3ODQ5YL9aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **모델 설정 및 평가**"
      ],
      "metadata": {
        "id": "_cX5z5eWNDDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
        "tf_test = vectorizer.transform(tokenized_texts_test)\n",
        "\n",
        "\n",
        "y_train = train['label'].values\n",
        "\n",
        "# 모델 설정\n",
        "bayes_model = MultinomialNB(alpha=0.02)\n",
        "sgd_model = SGDClassifier(max_iter=10000, tol=1e-4, loss=\"modified_huber\")\n",
        "\n",
        "# 앙상블중 voting 방법으로 모델 생성, 가중치 설정\n",
        "ensemble = VotingClassifier(estimators=[('sgd', sgd_model),\n",
        "                                        ('nb', bayes_model)],\n",
        "                            weights=[0.85, 0.15], voting='soft', n_jobs=-1)\n",
        "ensemble.fit(tf_train, y_train)\n",
        "\n",
        "# 최종 예측 결과물\n",
        "final_preds = ensemble.predict_proba(tf_test)[:,1]\n",
        "sub['generated'] = final_preds\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub"
      ],
      "metadata": {
        "id": "HFduIFUxMCsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face**를 이용해서  transformer 라이브러리 **PreTrainedTokenizerFast** 사용\n",
        "\n",
        "**Byte Pair Encoding(BPE)**\n",
        "\n",
        "텍스트 데이터에서 가장 빈번하게 발생하는 문자열을 고려하여 새로운 토큰으로 대체함으로써 텍스트를 압축하거나 토큰화\n",
        "\n",
        "**나이브 베이즈 분류**\n",
        "\n",
        "• 각 특성을 개별로 취급해 파라미터를 학습하고 그 특성에서 클래스별 통계를 단순하게 취합시킨다.\n",
        "\n",
        "**MultinomialNB** 클래스별로 특성의 평균을 계산\n",
        "\n",
        "(alpha - 가상의 데이터 포인트를 추가하여 통계 데이터를 완만하게 해준다.)\n",
        "\n",
        "**SGDClassifier**\n",
        "\n",
        "**loss=\"modified_huber\"**  \n",
        "\n",
        "이상치에 대한 허용성을 가져오면서도 확률 추정치에 대해 부드러운 손실 함수\n",
        "\n",
        "**ACC : 0.946**"
      ],
      "metadata": {
        "id": "aDYoyOFfNH8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고자료\n",
        "\n",
        "PreTrainedTokenizerFast\n",
        "https://bo-10000.tistory.com/131\n",
        "\n",
        "Byte Pair Encoding(BPE)\n",
        "https://wikidocs.net/22592\n",
        "\n",
        "SGD 파라미터\n",
        "https://codingsmu.tistory.com/97"
      ],
      "metadata": {
        "id": "YkOic6HJNLi8"
      }
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF8pQHp5MH4A"
   },
   "source": [
    "# **필요한 라이브러리 및 모듈**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8-_ijmhbLrRL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Location '../input/daigt-misc/' is ignored: it is either a non-existing path or lacks a specific scheme.\n",
      "ERROR: Could not find a version that satisfies the requirement language-tool-python (from versions: none)\n",
      "ERROR: No matching distribution found for language-tool-python\n",
      "명령 구문이 올바르지 않습니다.\n",
      "'cp'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q language-tool-python --no-index --find-links ../input/daigt-misc/\n",
    "!mkdir -p /root/.cache/language_tool_python/\n",
    "!cp -r /kaggle/input/daigt-misc/lang57/LanguageTool-5.7 /root/.cache/language_tool_python/LanguageTool-5.7\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import language_tool_python\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "seed = 202\n",
    "\n",
    "def seed_everything(seed=202):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7QNO0HzMif5"
   },
   "source": [
    "# **캐글 데이터 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIeY73tRLz79"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n",
    "external_train = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "external_train.rename(columns={'generated': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF7mOxhBM1Ay"
   },
   "source": [
    "# **전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KNTS7HoL4Zt"
   },
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "def correct_sentence(sentence):\n",
    "    return tool.correct(sentence)\n",
    "def correct_df(df):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        df['text'] = list(executor.map(correct_sentence, df['text']))\n",
    "        \n",
    "def how_many_typos(text):    \n",
    "    return len(tool.check(text))\n",
    "\n",
    "not_persuade_df = train[train['source'] != 'persuade_corpus']\n",
    "persuade_df = train[train['source'] == 'persuade_corpus']\n",
    "sampled_persuade_df = persuade_df.sample(n=6000, random_state=42)\n",
    "\n",
    "all_human = set(list(''.join(sampled_persuade_df.text.to_list())))\n",
    "other = set(list(''.join(not_persuade_df.text.to_list())))\n",
    "chars_to_remove = ''.join([x for x in other if x not in all_human])\n",
    "\n",
    "translation_table = str.maketrans('', '', chars_to_remove)\n",
    "def remove_chars(s):\n",
    "    return s.translate(translation_table)\n",
    "\n",
    "train=pd.concat([train,external_train])\n",
    "train['text'] = train['text'].apply(remove_chars)\n",
    "train['text'] = train['text'].str.replace('\\n', '')\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "test['text'] = test['text'].str.replace('\\n', '')\n",
    "test['text'] = test['text'].apply(remove_chars)\n",
    "correct_df(test)\n",
    "df = pd.concat([train['text'], test['text']], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdMWpw7eM9pf"
   },
   "source": [
    "# **단어 벡터화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC3ODQ5YL9aO"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5),tokenizer=lambda x: re.findall(r'[^\\W]+', x), token_pattern=None, strip_accents='unicode')\n",
    "vectorizer = vectorizer.fit(test['text'])\n",
    "X = vectorizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cX5z5eWNDDA"
   },
   "source": [
    "# **모델 설정 및 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFduIFUxMCsC"
   },
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression()\n",
    "lr2 = LogisticRegression()\n",
    "lr3 = LogisticRegression()\n",
    "\n",
    "clf1 = MultinomialNB(alpha=0.02)\n",
    "clf2 = MultinomialNB(alpha=0.02)\n",
    "clf3 = MultinomialNB(alpha=0.02)\n",
    "\n",
    "sgd_model1 = SGDClassifier(max_iter=8000, tol=1e-3, loss=\"modified_huber\")\n",
    "sgd_model2 = SGDClassifier(max_iter=10000, tol=5e-4, loss=\"modified_huber\", class_weight=\"balanced\") \n",
    "sgd_model3 = SGDClassifier(max_iter=15000, tol=3e-4, loss=\"modified_huber\", early_stopping=True)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr1', lr1), ('lr2', lr2), ('lr3', lr3), ('mnb1', clf1), ('mnb2', clf2), ('mnb3', clf3),\n",
    "                ('sgd1', sgd_model1), ('sgd2', sgd_model2), ('sgd3', sgd_model3)],\n",
    "    weights=[0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.15, 0.15],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X[:train.shape[0]], train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = ensemble.predict_proba(X[train.shape[0]:])[:,1]\n",
    "\n",
    "ntypos=test['text'].apply(lambda x: how_many_typos(x))\n",
    "test['ntypos'] = -ntypos\n",
    "test['generated'] = preds_test\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test[\"id\"],\n",
    "    'generated': test['generated']\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
